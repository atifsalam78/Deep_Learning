{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2443ecc-93a4-4ac6-b616-a3f09e7d98aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (3,3) (3,3,32,64) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 68\u001b[0m\n\u001b[0;32m     66\u001b[0m conv1 \u001b[38;5;241m=\u001b[39m relu(convolve(x, weights_conv1))  \n\u001b[0;32m     67\u001b[0m pool1 \u001b[38;5;241m=\u001b[39m max_pool(conv1, \u001b[38;5;241m2\u001b[39m)  \n\u001b[1;32m---> 68\u001b[0m conv2 \u001b[38;5;241m=\u001b[39m relu(convolve(pool1, weights_conv2))  \n\u001b[0;32m     69\u001b[0m pool2 \u001b[38;5;241m=\u001b[39m max_pool(conv2, \u001b[38;5;241m2\u001b[39m)  \n\u001b[0;32m     70\u001b[0m flat \u001b[38;5;241m=\u001b[39m flatten(pool2)  \n",
      "Cell \u001b[1;32mIn[1], line 31\u001b[0m, in \u001b[0;36mconvolve\u001b[1;34m(x, weights)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):  \n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]):  \n\u001b[1;32m---> 31\u001b[0m         output[i, j] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(x[i:i\u001b[38;5;241m+\u001b[39mweights\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], j:j\u001b[38;5;241m+\u001b[39mweights\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]] \u001b[38;5;241m*\u001b[39m weights)  \n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (3,3) (3,3,32,64) "
     ]
    }
   ],
   "source": [
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt  \n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.metrics import accuracy_score  \n",
    "from sklearn.utils import shuffle  \n",
    "\n",
    "# Load MNIST dataset  \n",
    "from tensorflow.keras.datasets import mnist  \n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()  \n",
    "\n",
    "# Reshape and normalize data  \n",
    "X_train = X_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0  \n",
    "X_test = X_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0  \n",
    "\n",
    "# Split data into training and validation sets  \n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)  \n",
    "\n",
    "# Define ReLU activation function  \n",
    "def relu(x):  \n",
    "    return np.maximum(x, 0)  \n",
    "\n",
    "# Define softmax activation function  \n",
    "def softmax(x):  \n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)  \n",
    "\n",
    "# Define convolution operation  \n",
    "def convolve(x, weights):  \n",
    "    output = np.zeros((x.shape[0] - weights.shape[0] + 1, x.shape[1] - weights.shape[1] + 1))  \n",
    "    for i in range(output.shape[0]):  \n",
    "        for j in range(output.shape[1]):  \n",
    "            output[i, j] = np.sum(x[i:i+weights.shape[0], j:j+weights.shape[1]] * weights)  \n",
    "    return output  \n",
    "\n",
    "# Define max pooling operation  \n",
    "def max_pool(x, size):  \n",
    "    output = np.zeros((x.shape[0] // size, x.shape[1] // size))  \n",
    "    for i in range(output.shape[0]):  \n",
    "        for j in range(output.shape[1]):  \n",
    "            output[i, j] = np.max(x[i*size:(i+1)*size, j*size:(j+1)*size])  \n",
    "    return output  \n",
    "\n",
    "# Define flatten operation  \n",
    "def flatten(x):  \n",
    "    return x.reshape(x.shape[0], -1)  \n",
    "\n",
    "# Define dense layer operation  \n",
    "def dense(x, weights, bias):  \n",
    "    return np.dot(x, weights) + bias  \n",
    "\n",
    "# Initialize weights and biases  \n",
    "weights_conv1 = np.random.rand(3, 3, 1, 32)  \n",
    "weights_conv2 = np.random.rand(3, 3, 32, 64)  \n",
    "weights_dense1 = np.random.rand(7*7*64, 128)  \n",
    "weights_dense2 = np.random.rand(128, 10)  \n",
    "bias_conv1 = np.zeros((32,))  \n",
    "bias_conv2 = np.zeros((64,))  \n",
    "bias_dense1 = np.zeros((128,))  \n",
    "bias_dense2 = np.zeros((10,))  \n",
    "\n",
    "# Train the model  \n",
    "for epoch in range(10):  \n",
    "    X_train, y_train = shuffle(X_train, y_train)  \n",
    "    for i in range(0, len(X_train), 32):  \n",
    "        # Forward pass  \n",
    "        x = X_train[i:i+32]  \n",
    "        conv1 = relu(convolve(x, weights_conv1))  \n",
    "        pool1 = max_pool(conv1, 2)  \n",
    "        conv2 = relu(convolve(pool1, weights_conv2))  \n",
    "        pool2 = max_pool(conv2, 2)  \n",
    "        flat = flatten(pool2)  \n",
    "        dense1 = relu(dense(flat, weights_dense1, bias_dense1))  \n",
    "        output = softmax(dense(dense1, weights_dense2, bias_dense2))  \n",
    "\n",
    "        # Backward pass  \n",
    "        loss = -np.mean(np.log(output[np.arange(32), y_train[i:i+32]]))  \n",
    "        d_output = output  \n",
    "        d_output[np.arange(32), y_train[i:i+32]] -= 1  \n",
    "        d_dense2 = d_output  \n",
    "        d_dense1 = np.dot(d_dense2, weights_dense2.T)  \n",
    "        d_flat = np.dot(d_dense1, weights_dense1.T)  \n",
    "        d_pool2 = d_flat.reshape(-1, 7, 7, 64)  \n",
    "        d_conv2 = np.zeros((d_pool2.shape[0], 14, 14, 64))  \n",
    "        for j in range(d_pool2.shape[1]):  \n",
    "            for k in range(d_pool2.shape[2]):  \n",
    "                d_conv2[:, j*2:j*2+2, k*2:k*2+2, :] += d_pool2[:, j, k, :]  \n",
    "        d_conv2 = d_conv2[:, 1:-1, 1:-1, :]  \n",
    "        d_conv2 = np.multiply(d_conv2, np.where(conv2 > 0, 1, 0))  \n",
    "        d_weights_conv2 = np.zeros((3, 3, 32, 64))  \n",
    "        for j in range(d_conv2.shape[1]):  \n",
    "            for k in range(d_conv2.shape[2]):  \n",
    "                d_weights_conv2 += np.dot(d_conv2[:, j:j+3, k:k+3, :].reshape(-1, 1), pool1[:, j:j+3, k:k+3, :].reshape(-1, 1).T)  \n",
    "        d_pool1 = np.zeros((d_conv2.shape[0], 28, 28, 32))  \n",
    "        for j in range(d_conv2.shape[1]):  \n",
    "            for k in range(d_conv2.shape[2]):  \n",
    "                d_pool1[:, j*2:j*2+2, k*2:k*2+2, :] += d_conv2[:, j, k, :]  \n",
    "        d_conv1 = np.zeros((d_pool1.shape[0], 30, 30, 32))  \n",
    "        for j in range(d_pool1.shape[1]):  \n",
    "            for k in range(d_pool1.shape[2]):  \n",
    "                d_conv1[:, j:j+3, k:k+3, :] += d_pool1[:, j, k, :]  \n",
    "        d_conv1 = d_conv1[:, 1:-1, 1:-1, :]  \n",
    "        d_conv1 = np.multiply(d_conv1, np.where(conv1 > 0, 1, 0))  \n",
    "        d_weights_conv1 = np.zeros((3, 3, 1, 32))  \n",
    "        for j in range(d_conv1.shape[1]):  \n",
    "            for k in range(d_conv1.shape[2]):  \n",
    "                d_weights_conv1 += np.dot(d_conv1[:, j:j+3, k:k+3, :].reshape(-1, 1), x[:, j:j+3, k:k+3, :].reshape(-1, 1).T)  \n",
    "\n",
    "        # Weight update  \n",
    "        weights_conv1 -= 0.01 * d_weights_conv1  \n",
    "        weights_conv2 -= 0.01 * d_weights_conv2  \n",
    "        weights_dense1 -= 0.01 * np.dot(flat.T, d_dense1)  \n",
    "        weights_dense2 -= 0.01 * np.dot(dense1.T, d_dense2)  \n",
    "        bias_conv1 -= 0.01 * np.sum(d_conv1, axis=(0, 1, 2))  \n",
    "        bias_conv2 -= 0.01 * np.sum(d_conv2, axis=(0, 1, 2))  \n",
    "        bias_dense1 -= 0.01 * np.sum(d_dense1, axis=0)  \n",
    "        bias_dense2 -= 0.01 * np.sum(d_dense2, axis=0)  \n",
    "\n",
    "    print('Epoch', epoch+1, 'Loss:', loss)  \n",
    "\n",
    "# Evaluate the model  \n",
    "conv1 = relu(convolve(X_val, weights_conv1))  \n",
    "pool1 = max_pool(conv1, 2)  \n",
    "conv2 = relu(convolve(pool1, weights_conv2))  \n",
    "pool2 = max_pool(conv2, 2)  \n",
    "flat = flatten(pool2)  \n",
    "dense1 = relu(dense(flat, weights_dense1, bias_dense1))  \n",
    "output = softmax(dense(dense1, weights_dense2, bias_dense2))  \n",
    "print('Validation Accuracy:', accuracy_score(y_val, np.argmax(output, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4400b4-eeac-458d-ad90-5379b9d0f15a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
