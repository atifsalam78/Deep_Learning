{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf0376a6-1791-449d-bab7-6effbefa2924",
   "metadata": {},
   "source": [
    "**Feed Forward Network**\n",
    "\n",
    "A feed forward network, also known as a feed forward neural network (FFNN), is a type of artificial neural network where the data flows only in one direction, from input layer to output layer, without any feedback loops or cycles. In other words, the data flows \"forward\" through the network, from the input layer to the hidden layers (if any) and finally to the output layer.\n",
    "\n",
    "Here's a high-level overview of a feed forward network:\n",
    "\n",
    "1. **Input Layer**: The input layer receives the input data, which is then propagated to the next layer.\n",
    "2. **Hidden Layers**: One or more hidden layers process the input data through complex transformations, using activation functions such as sigmoid, ReLU, or tanh.\n",
    "3. **Output Layer**: The final output layer generates the predicted output based on the processed data from the hidden layers.\n",
    "\n",
    "**Backward Propagation**\n",
    "\n",
    "Backward propagation, also known as backpropagation, is an essential algorithm used to train feed forward neural networks. It's a method for computing the gradients of the loss function with respect to the model's parameters, which is necessary for optimizing the model's performance.\n",
    "\n",
    "Here's a step-by-step overview of the backward propagation algorithm:\n",
    "\n",
    "1. **Forward Pass**: The input data is propagated through the network, and the predicted output is generated.\n",
    "2. **Error Calculation**: The difference between the predicted output and the actual output (target) is calculated, resulting in an error or loss.\n",
    "3. **Backward Pass**: The error is propagated backwards through the network, from the output layer to the input layer, to compute the gradients of the loss function with respect to each layer's weights and biases.\n",
    "4. **Weight Update**: The gradients are used to update the model's weights and biases, using an optimization algorithm such as stochastic gradient descent (SGD), Adam, or RMSProp.\n",
    "\n",
    "The backward propagation algorithm is based on the chain rule of calculus, which allows us to compute the gradients of the loss function with respect to the model's parameters. This process is repeated iteratively, with the model's weights and biases being updated after each iteration, until convergence or a stopping criterion is reached.\n",
    "\n",
    "**Example Code**\n",
    "\n",
    "Here's a simple example of a feed forward network with backward propagation using Python and the NumPy library:\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Define the number of inputs, hidden units, and outputs\n",
    "n_inputs = 2\n",
    "n_hidden = 2\n",
    "n_outputs = 1\n",
    "\n",
    "# Initialize the weights and biases\n",
    "weights1 = np.random.rand(n_inputs, n_hidden)\n",
    "weights2 = np.random.rand(n_hidden, n_outputs)\n",
    "bias1 = np.zeros((1, n_hidden))\n",
    "bias2 = np.zeros((1, n_outputs))\n",
    "\n",
    "# Define the activation functions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Define the forward pass\n",
    "def forward_pass(inputs):\n",
    "    hidden_layer = sigmoid(np.dot(inputs, weights1) + bias1)\n",
    "    output_layer = sigmoid(np.dot(hidden_layer, weights2) + bias2)\n",
    "    return hidden_layer, output_layer\n",
    "\n",
    "# Define the backward pass\n",
    "def backward_pass(inputs, targets, hidden_layer, output_layer):\n",
    "    error = targets - output_layer\n",
    "    d_output_layer = error * sigmoid_derivative(output_layer)\n",
    "    d_hidden_layer = d_output_layer.dot(weights2.T) * sigmoid_derivative(hidden_layer)\n",
    "    d_weights2 = hidden_layer.T.dot(d_output_layer)\n",
    "    d_weights1 = inputs.T.dot(d_hidden_layer)\n",
    "    d_bias2 = np.sum(d_output_layer, axis=0, keepdims=True)\n",
    "    d_bias1 = np.sum(d_hidden_layer, axis=0, keepdims=True)\n",
    "    return d_weights1, d_weights2, d_bias1, d_bias2\n",
    "\n",
    "# Train the network\n",
    "for epoch in range(1000):\n",
    "    inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "    targets = np.array([[0], [1], [1], [0]])\n",
    "    hidden_layer, output_layer = forward_pass(inputs)\n",
    "    d_weights1, d_weights2, d_bias1, d_bias2 = backward_pass(inputs, targets, hidden_layer, output_layer)\n",
    "    weights1 += 0.1 * d_weights1\n",
    "    weights2 += 0.1 * d_weights2\n",
    "    bias1 += 0.1 * d_bias1\n",
    "    bias2 += 0.1 * d_bias2\n",
    "    print(\"Epoch:\", epoch, \"Error:\", np.mean(np.abs(targets - output_layer)))\n",
    "```\n",
    "This code defines a simple feed forward network with two inputs, two hidden units, and one output. The network is trained using the backward propagation algorithm to learn the XOR function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4667b3-9af2-4d5c-a4ce-0df274cdab4e",
   "metadata": {},
   "source": [
    "**Activation Function**\n",
    "======================\n",
    "\n",
    "An activation function is a mathematical function that is applied to the output of a neural network node or layer. It is used to introduce non-linearity into the model, allowing it to learn and represent more complex relationships between the inputs and outputs.\n",
    "\n",
    "**Why We Use Activation Functions**\n",
    "----------------------------------\n",
    "\n",
    "1. **Introduce Non-Linearity**: Activation functions introduce non-linearity into the model, allowing it to learn and represent more complex relationships between the inputs and outputs. Without non-linearity, the model would only be able to learn linear relationships.\n",
    "2. **Improve Model Capacity**: Activation functions increase the capacity of the model, allowing it to fit more complex data distributions.\n",
    "3. **Allow Backpropagation**: Activation functions are required for backpropagation, which is the process of adjusting the model's parameters to minimize the error.\n",
    "\n",
    "**Three Common Activation Functions**\n",
    "-------------------------------------\n",
    "\n",
    "### 1. Sigmoid Activation Function\n",
    "\n",
    "The sigmoid activation function maps the input to a value between 0 and 1.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "```\n",
    "\n",
    "### 2. ReLU (Rectified Linear Unit) Activation Function\n",
    "\n",
    "The ReLU activation function maps all negative values to 0 and all positive values to the same value.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "```\n",
    "\n",
    "### 3. Tanh (Hyperbolic Tangent) Activation Function\n",
    "\n",
    "The tanh activation function maps the input to a value between -1 and 1.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "```\n",
    "\n",
    "**Code Example**\n",
    "---------------\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Example input\n",
    "x = np.array([1, 2, 3, 4, 5])\n",
    "\n",
    "# Apply sigmoid activation function\n",
    "output_sigmoid = 1 / (1 + np.exp(-x))\n",
    "print(\"Sigmoid Output:\", output_sigmoid)\n",
    "\n",
    "# Apply ReLU activation function\n",
    "output_relu = np.maximum(0, x)\n",
    "print(\"ReLU Output:\", output_relu)\n",
    "\n",
    "# Apply tanh activation function\n",
    "output_tanh = np.tanh(x)\n",
    "print(\"Tanh Output:\", output_tanh)\n",
    "```\n",
    "\n",
    "In this example, we apply the sigmoid, ReLU, and tanh activation functions to a simple input array `x`. The output of each activation function is different, but all three functions introduce non-linearity into the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb44b068-9ee3-456c-93c7-2c498c77b8ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
